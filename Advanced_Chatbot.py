import streamlit as st
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import requests
import os
import spacy
import time

# GitHub directory containing the DistilGPT2 model files
GITHUB_MODEL_URL = "https://github.com/MarpakaPradeepSai/Advanced-Events-Ticketing-Customer-Support-Chatbot/raw/main/DistilGPT2_Model"

# List of model files to download
MODEL_FILES = [
    "config.json",
    "generation_config.json",
    "merges.txt",
    "model.safetensors",
    "special_tokens_map.json",
    "tokenizer_config.json",
    "vocab.json"
]

# Function to download model files from GitHub
def download_model_files(model_dir="/tmp/DistilGPT2_Model"):
    os.makedirs(model_dir, exist_ok=True)
    
    for filename in MODEL_FILES:
        url = f"{GITHUB_MODEL_URL}/{filename}"
        local_path = os.path.join(model_dir, filename)
        
        if not os.path.exists(local_path):
            response = requests.get(url)
            if response.status_code == 200:
                with open(local_path, "wb") as f:
                    f.write(response.content)
            else:
                st.error(f"Failed to download {filename} from GitHub.")
                return False
    return True

# Load spaCy model for NER
@st.cache_resource
def load_spacy_model():
    nlp = spacy.load("en_core_web_trf")
    return nlp

# Load the DistilGPT2 model and tokenizer
@st.cache_resource(show_spinner=False)
def load_model_and_tokenizer():
    model_dir = "/tmp/DistilGPT2_Model"
    if not download_model_files(model_dir):
        st.error("Model download failed. Check your internet connection or GitHub URL.")
        return None, None
    
    model = GPT2LMHeadModel.from_pretrained(model_dir, trust_remote_code=True)
    tokenizer = GPT2Tokenizer.from_pretrained(model_dir)
    return model, tokenizer

# Replace placeholders in response
def replace_placeholders(response, dynamic_placeholders, static_placeholders):
    for placeholder, value in static_placeholders.items():
        response = response.replace(placeholder, value)
    for placeholder, value in dynamic_placeholders.items():
        response = response.replace(placeholder, value)
    return response

# Extract dynamic placeholders using spaCy
def extract_dynamic_placeholders(user_question, nlp):
    doc = nlp(user_question)
    dynamic_placeholders = {}

    for ent in doc.ents:
        if ent.label_ == "EVENT":
            dynamic_placeholders['{{EVENT}}'] = f"<b>{ent.text.title()}</b>"
        elif ent.label_ == "GPE":
            dynamic_placeholders['{{CITY}}'] = f"<b>{ent.text.title()}</b>"

    if '{{EVENT}}' not in dynamic_placeholders:
        dynamic_placeholders['{{EVENT}}'] = "event"
    if '{{CITY}}' not in dynamic_placeholders:
        dynamic_placeholders['{{CITY}}'] = "city"

    return dynamic_placeholders

# Generate response using DistilGPT2
def generate_response(model, tokenizer, instruction, max_length=256):
    model.eval()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    input_text = f"Instruction: {instruction} Response:"
    
    inputs = tokenizer(input_text, return_tensors="pt", padding=True).to(device)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_length=max_length,
            num_return_sequences=1,
            temperature=0.7,
            top_p=0.95,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response_start = response.find("Response:") + len("Response:")
    return response[response_start:].strip()

# Streamlit UI
st.markdown("<h1 style='font-size: 43px;'>Advanced Events Ticketing Chatbot</h1>", unsafe_allow_html=True)
st.write("Ask me about ticket cancellations, refunds, or any event-related inquiries!")

nlp = load_spacy_model()
model, tokenizer = load_model_and_tokenizer()

if model is None or tokenizer is None:
    st.error("Failed to load the model.")
    st.stop()

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

for message in st.session_state.chat_history:
    with st.chat_message(message["role"], avatar=message["avatar"]):
        st.markdown(message["content"], unsafe_allow_html=True)

if prompt := st.chat_input("Enter your question:"):
    prompt = prompt[0].upper() + prompt[1:] if prompt else prompt

    if not prompt.strip():
        st.session_state.chat_history.append({"role": "user", "content": prompt, "avatar": "üë§"})
        with st.chat_message("user", avatar="üë§"):
            st.markdown(prompt, unsafe_allow_html=True)
        with st.chat_message("assistant", avatar="ü§ñ"):
            st.error("‚ö†Ô∏è Please enter a valid question.")
        st.session_state.chat_history.append({
            "role": "assistant", "content": "Please enter a valid question.", "avatar": "ü§ñ"
        })
    else:
        st.session_state.chat_history.append({"role": "user", "content": prompt, "avatar": "üë§"})
        with st.chat_message("user", avatar="üë§"):
            st.markdown(prompt, unsafe_allow_html=True)

        with st.chat_message("assistant", avatar="ü§ñ"):
            with st.spinner("Generating response..."):
                time.sleep(0.5)  # Simulate processing delay

                dynamic_placeholders = extract_dynamic_placeholders(prompt, nlp)
                response = generate_response(model, tokenizer, prompt)
                full_response = replace_placeholders(response, dynamic_placeholders, static_placeholders)

                st.markdown(full_response, unsafe_allow_html=True)
                st.session_state.chat_history.append({
                    "role": "assistant", "content": full_response, "avatar": "ü§ñ"
                })

if st.session_state.chat_history:
    if st.button("Reset Chat"):
        st.session_state.chat_history = []
        st.experimental_rerun()
